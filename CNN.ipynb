{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77b09556-535e-43ed-b518-75660937ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a738127-7c89-465c-9c82-9ff325033196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        # Filter for image files only (e.g., .jpg, .png)\n",
    "        self.images = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx].replace('.jpg', '.png').replace('.png', '.png'))\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        # Convert mask to binary (0 or 1)\n",
    "        mask = (mask > 0.5).float()\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b361d66-fed3-42b1-93b3-1548aa8f1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed7e070e-97d4-4030-86f2-7940928b4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "full_dataset = SegmentationDataset(\n",
    "    image_dir=\"../imagespng/imagespng\",\n",
    "    mask_dir=\"../binarypng/binarypng\",\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490c9263-3b46-4c40-ae2e-1ec35ae37c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets (80% train, 20% val)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315bb070-0485-484a-ada1-25c964d2134c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calviet/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/calviet/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = deeplabv3_resnet50(pretrained=False, num_classes=1)  # Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb479c76-5e83-4a23-8ba1-24b10e69b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58720dee-aee0-4288-a01e-53b43dc4cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39c8d046-8808-4e7c-a8ed-e9ccb6a438db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images, masks = images.to(device), masks.to(device)  # Move to device\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)['out']\n",
    "            outputs = outputs.squeeze(2)  # Remove the extra dimension (squeeze the 2nd dimension)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 10 == 0:  # Print every 10 batches\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Avg Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation step\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)  # Move to device\n",
    "            outputs = model(images)['out']\n",
    "            outputs = outputs.squeeze(2)  # Remove the extra dimension\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86dd511d-d49b-40b5-821c-4179f92e4943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 1/494, Loss: 0.7496\n",
      "Epoch 1/10, Batch 11/494, Loss: 0.1745\n",
      "Epoch 1/10, Batch 21/494, Loss: 0.0877\n",
      "Epoch 1/10, Batch 31/494, Loss: 0.0645\n",
      "Epoch 1/10, Batch 41/494, Loss: 0.0514\n",
      "Epoch 1/10, Batch 51/494, Loss: 0.0506\n",
      "Epoch 1/10, Batch 61/494, Loss: 0.0394\n",
      "Epoch 1/10, Batch 71/494, Loss: 0.0357\n",
      "Epoch 1/10, Batch 81/494, Loss: 0.0357\n",
      "Epoch 1/10, Batch 91/494, Loss: 0.0350\n",
      "Epoch 1/10, Batch 101/494, Loss: 0.0272\n",
      "Epoch 1/10, Batch 111/494, Loss: 0.0309\n",
      "Epoch 1/10, Batch 121/494, Loss: 0.0218\n",
      "Epoch 1/10, Batch 131/494, Loss: 0.0313\n",
      "Epoch 1/10, Batch 141/494, Loss: 0.0182\n",
      "Epoch 1/10, Batch 151/494, Loss: 0.0207\n",
      "Epoch 1/10, Batch 161/494, Loss: 0.0286\n",
      "Epoch 1/10, Batch 171/494, Loss: 0.0262\n",
      "Epoch 1/10, Batch 181/494, Loss: 0.0224\n",
      "Epoch 1/10, Batch 191/494, Loss: 0.0208\n",
      "Epoch 1/10, Batch 201/494, Loss: 0.0186\n",
      "Epoch 1/10, Batch 211/494, Loss: 0.0172\n",
      "Epoch 1/10, Batch 221/494, Loss: 0.0158\n",
      "Epoch 1/10, Batch 231/494, Loss: 0.0249\n",
      "Epoch 1/10, Batch 241/494, Loss: 0.0244\n",
      "Epoch 1/10, Batch 251/494, Loss: 0.0173\n",
      "Epoch 1/10, Batch 261/494, Loss: 0.0206\n",
      "Epoch 1/10, Batch 271/494, Loss: 0.0146\n",
      "Epoch 1/10, Batch 281/494, Loss: 0.0208\n",
      "Epoch 1/10, Batch 291/494, Loss: 0.0183\n",
      "Epoch 1/10, Batch 301/494, Loss: 0.0228\n",
      "Epoch 1/10, Batch 311/494, Loss: 0.0142\n",
      "Epoch 1/10, Batch 321/494, Loss: 0.0180\n",
      "Epoch 1/10, Batch 331/494, Loss: 0.0188\n",
      "Epoch 1/10, Batch 341/494, Loss: 0.0119\n",
      "Epoch 1/10, Batch 351/494, Loss: 0.0193\n",
      "Epoch 1/10, Batch 361/494, Loss: 0.0163\n",
      "Epoch 1/10, Batch 371/494, Loss: 0.0206\n",
      "Epoch 1/10, Batch 381/494, Loss: 0.0193\n",
      "Epoch 1/10, Batch 391/494, Loss: 0.0130\n",
      "Epoch 1/10, Batch 401/494, Loss: 0.0155\n",
      "Epoch 1/10, Batch 411/494, Loss: 0.0173\n",
      "Epoch 1/10, Batch 421/494, Loss: 0.0111\n",
      "Epoch 1/10, Batch 431/494, Loss: 0.0211\n",
      "Epoch 1/10, Batch 441/494, Loss: 0.0153\n",
      "Epoch 1/10, Batch 451/494, Loss: 0.0163\n",
      "Epoch 1/10, Batch 461/494, Loss: 0.0186\n",
      "Epoch 1/10, Batch 471/494, Loss: 0.0177\n",
      "Epoch 1/10, Batch 481/494, Loss: 0.0168\n",
      "Epoch 1/10, Batch 491/494, Loss: 0.0144\n",
      "Epoch 1/10, Avg Training Loss: 0.0331\n",
      "Epoch 1/10, Validation Loss: 0.0144\n",
      "Epoch 2/10, Batch 1/494, Loss: 0.0142\n",
      "Epoch 2/10, Batch 11/494, Loss: 0.0161\n",
      "Epoch 2/10, Batch 21/494, Loss: 0.0124\n",
      "Epoch 2/10, Batch 31/494, Loss: 42.1927\n",
      "Epoch 2/10, Batch 41/494, Loss: 0.2240\n",
      "Epoch 2/10, Batch 51/494, Loss: 0.2125\n",
      "Epoch 2/10, Batch 61/494, Loss: 0.3129\n",
      "Epoch 2/10, Batch 71/494, Loss: 0.3586\n",
      "Epoch 2/10, Batch 81/494, Loss: 0.3192\n",
      "Epoch 2/10, Batch 91/494, Loss: 0.1504\n",
      "Epoch 2/10, Batch 101/494, Loss: 0.2694\n",
      "Epoch 2/10, Batch 111/494, Loss: 0.2467\n",
      "Epoch 2/10, Batch 121/494, Loss: 0.2105\n",
      "Epoch 2/10, Batch 131/494, Loss: 0.2333\n",
      "Epoch 2/10, Batch 141/494, Loss: 0.2334\n",
      "Epoch 2/10, Batch 151/494, Loss: 0.2233\n",
      "Epoch 2/10, Batch 161/494, Loss: 0.1337\n",
      "Epoch 2/10, Batch 171/494, Loss: 0.1421\n",
      "Epoch 2/10, Batch 181/494, Loss: 0.1003\n",
      "Epoch 2/10, Batch 191/494, Loss: 0.1037\n",
      "Epoch 2/10, Batch 201/494, Loss: 0.0895\n",
      "Epoch 2/10, Batch 211/494, Loss: 0.1268\n",
      "Epoch 2/10, Batch 221/494, Loss: 0.0883\n",
      "Epoch 2/10, Batch 231/494, Loss: 0.0688\n",
      "Epoch 2/10, Batch 241/494, Loss: 0.0823\n",
      "Epoch 2/10, Batch 251/494, Loss: 0.0321\n",
      "Epoch 2/10, Batch 261/494, Loss: 0.0387\n",
      "Epoch 2/10, Batch 271/494, Loss: 0.0464\n",
      "Epoch 2/10, Batch 281/494, Loss: 0.0193\n",
      "Epoch 2/10, Batch 291/494, Loss: 0.0402\n",
      "Epoch 2/10, Batch 301/494, Loss: 0.0273\n",
      "Epoch 2/10, Batch 311/494, Loss: 0.0401\n",
      "Epoch 2/10, Batch 321/494, Loss: 0.0337\n",
      "Epoch 2/10, Batch 331/494, Loss: 0.0384\n",
      "Epoch 2/10, Batch 341/494, Loss: 0.0502\n",
      "Epoch 2/10, Batch 351/494, Loss: 0.0372\n",
      "Epoch 2/10, Batch 361/494, Loss: 0.0357\n",
      "Epoch 2/10, Batch 371/494, Loss: 0.0341\n",
      "Epoch 2/10, Batch 381/494, Loss: 0.0440\n",
      "Epoch 2/10, Batch 391/494, Loss: 0.0328\n",
      "Epoch 2/10, Batch 401/494, Loss: 0.0260\n",
      "Epoch 2/10, Batch 411/494, Loss: 0.0502\n",
      "Epoch 2/10, Batch 421/494, Loss: 0.0298\n",
      "Epoch 2/10, Batch 431/494, Loss: 0.0379\n",
      "Epoch 2/10, Batch 441/494, Loss: 0.0336\n",
      "Epoch 2/10, Batch 451/494, Loss: 0.0304\n",
      "Epoch 2/10, Batch 461/494, Loss: 0.0353\n",
      "Epoch 2/10, Batch 471/494, Loss: 0.0486\n",
      "Epoch 2/10, Batch 481/494, Loss: 0.0225\n",
      "Epoch 2/10, Batch 491/494, Loss: 0.0420\n",
      "Epoch 2/10, Avg Training Loss: 0.4621\n",
      "Epoch 2/10, Validation Loss: 0.0327\n",
      "Epoch 3/10, Batch 1/494, Loss: 0.0304\n",
      "Epoch 3/10, Batch 11/494, Loss: 0.0259\n",
      "Epoch 3/10, Batch 21/494, Loss: 0.0316\n",
      "Epoch 3/10, Batch 31/494, Loss: 0.0383\n",
      "Epoch 3/10, Batch 41/494, Loss: 0.0327\n",
      "Epoch 3/10, Batch 51/494, Loss: 0.0252\n",
      "Epoch 3/10, Batch 61/494, Loss: 0.0351\n",
      "Epoch 3/10, Batch 71/494, Loss: 0.0241\n",
      "Epoch 3/10, Batch 81/494, Loss: 0.0305\n",
      "Epoch 3/10, Batch 91/494, Loss: 0.0335\n",
      "Epoch 3/10, Batch 101/494, Loss: 0.0362\n",
      "Epoch 3/10, Batch 111/494, Loss: 0.0306\n",
      "Epoch 3/10, Batch 121/494, Loss: 0.0331\n",
      "Epoch 3/10, Batch 131/494, Loss: 0.0388\n",
      "Epoch 3/10, Batch 141/494, Loss: 0.0437\n",
      "Epoch 3/10, Batch 151/494, Loss: 0.0276\n",
      "Epoch 3/10, Batch 161/494, Loss: 0.0297\n",
      "Epoch 3/10, Batch 171/494, Loss: 0.0315\n",
      "Epoch 3/10, Batch 181/494, Loss: 0.0190\n",
      "Epoch 3/10, Batch 191/494, Loss: 0.0203\n",
      "Epoch 3/10, Batch 201/494, Loss: 0.0265\n",
      "Epoch 3/10, Batch 211/494, Loss: 0.0291\n",
      "Epoch 3/10, Batch 221/494, Loss: 0.0365\n",
      "Epoch 3/10, Batch 231/494, Loss: 0.0243\n",
      "Epoch 3/10, Batch 241/494, Loss: 0.0303\n",
      "Epoch 3/10, Batch 251/494, Loss: 0.0299\n",
      "Epoch 3/10, Batch 261/494, Loss: 0.0192\n",
      "Epoch 3/10, Batch 271/494, Loss: 0.0211\n",
      "Epoch 3/10, Batch 281/494, Loss: 0.0139\n",
      "Epoch 3/10, Batch 291/494, Loss: 0.0260\n",
      "Epoch 3/10, Batch 301/494, Loss: 0.0168\n",
      "Epoch 3/10, Batch 311/494, Loss: 0.0230\n",
      "Epoch 3/10, Batch 321/494, Loss: 0.0197\n",
      "Epoch 3/10, Batch 331/494, Loss: 0.0286\n",
      "Epoch 3/10, Batch 341/494, Loss: 0.0185\n",
      "Epoch 3/10, Batch 351/494, Loss: 0.0157\n",
      "Epoch 3/10, Batch 361/494, Loss: 0.0288\n",
      "Epoch 3/10, Batch 371/494, Loss: 0.0190\n",
      "Epoch 3/10, Batch 381/494, Loss: 0.0188\n",
      "Epoch 3/10, Batch 391/494, Loss: 0.0244\n",
      "Epoch 3/10, Batch 401/494, Loss: 0.0203\n",
      "Epoch 3/10, Batch 411/494, Loss: 0.0222\n",
      "Epoch 3/10, Batch 421/494, Loss: 0.0276\n",
      "Epoch 3/10, Batch 431/494, Loss: 0.0260\n",
      "Epoch 3/10, Batch 441/494, Loss: 0.0335\n",
      "Epoch 3/10, Batch 451/494, Loss: 0.0224\n",
      "Epoch 3/10, Batch 461/494, Loss: 0.0309\n",
      "Epoch 3/10, Batch 471/494, Loss: 0.0204\n",
      "Epoch 3/10, Batch 481/494, Loss: 0.0262\n",
      "Epoch 3/10, Batch 491/494, Loss: 0.0165\n",
      "Epoch 3/10, Avg Training Loss: 0.0257\n",
      "Epoch 3/10, Validation Loss: 0.0216\n",
      "Epoch 4/10, Batch 1/494, Loss: 0.0237\n",
      "Epoch 4/10, Batch 11/494, Loss: 0.0230\n",
      "Epoch 4/10, Batch 21/494, Loss: 0.0110\n",
      "Epoch 4/10, Batch 31/494, Loss: 0.0222\n",
      "Epoch 4/10, Batch 41/494, Loss: 0.0113\n",
      "Epoch 4/10, Batch 51/494, Loss: 0.0172\n",
      "Epoch 4/10, Batch 61/494, Loss: 0.0227\n",
      "Epoch 4/10, Batch 71/494, Loss: 0.0219\n",
      "Epoch 4/10, Batch 81/494, Loss: 0.0219\n",
      "Epoch 4/10, Batch 91/494, Loss: 0.0201\n",
      "Epoch 4/10, Batch 101/494, Loss: 0.0283\n",
      "Epoch 4/10, Batch 111/494, Loss: 0.0314\n",
      "Epoch 4/10, Batch 121/494, Loss: 0.0139\n",
      "Epoch 4/10, Batch 131/494, Loss: 0.0076\n",
      "Epoch 4/10, Batch 141/494, Loss: 0.0209\n",
      "Epoch 4/10, Batch 151/494, Loss: 0.0165\n",
      "Epoch 4/10, Batch 161/494, Loss: 0.0221\n",
      "Epoch 4/10, Batch 171/494, Loss: 0.0212\n",
      "Epoch 4/10, Batch 181/494, Loss: 0.0212\n",
      "Epoch 4/10, Batch 191/494, Loss: 0.0209\n",
      "Epoch 4/10, Batch 201/494, Loss: 0.0171\n",
      "Epoch 4/10, Batch 211/494, Loss: 0.0225\n",
      "Epoch 4/10, Batch 221/494, Loss: 0.0181\n",
      "Epoch 4/10, Batch 231/494, Loss: 0.0171\n",
      "Epoch 4/10, Batch 241/494, Loss: 0.0230\n",
      "Epoch 4/10, Batch 251/494, Loss: 0.0213\n",
      "Epoch 4/10, Batch 261/494, Loss: 0.0269\n",
      "Epoch 4/10, Batch 271/494, Loss: 0.0206\n",
      "Epoch 4/10, Batch 281/494, Loss: 0.0189\n",
      "Epoch 4/10, Batch 291/494, Loss: 0.0227\n",
      "Epoch 4/10, Batch 301/494, Loss: 0.0135\n",
      "Epoch 4/10, Batch 311/494, Loss: 0.0181\n",
      "Epoch 4/10, Batch 321/494, Loss: 0.0242\n",
      "Epoch 4/10, Batch 331/494, Loss: 0.0198\n",
      "Epoch 4/10, Batch 341/494, Loss: 0.0256\n",
      "Epoch 4/10, Batch 351/494, Loss: 0.0205\n",
      "Epoch 4/10, Batch 361/494, Loss: 0.0235\n",
      "Epoch 4/10, Batch 371/494, Loss: 0.0128\n",
      "Epoch 4/10, Batch 381/494, Loss: 0.0181\n",
      "Epoch 4/10, Batch 391/494, Loss: 0.0234\n",
      "Epoch 4/10, Batch 401/494, Loss: 0.0216\n",
      "Epoch 4/10, Batch 411/494, Loss: 0.0183\n",
      "Epoch 4/10, Batch 421/494, Loss: 0.0255\n",
      "Epoch 4/10, Batch 431/494, Loss: 0.0178\n",
      "Epoch 4/10, Batch 441/494, Loss: 0.0205\n",
      "Epoch 4/10, Batch 451/494, Loss: 0.0168\n",
      "Epoch 4/10, Batch 461/494, Loss: 0.0210\n",
      "Epoch 4/10, Batch 471/494, Loss: 0.0127\n",
      "Epoch 4/10, Batch 481/494, Loss: 0.0109\n",
      "Epoch 4/10, Batch 491/494, Loss: 0.0163\n",
      "Epoch 4/10, Avg Training Loss: 0.0190\n",
      "Epoch 4/10, Validation Loss: 0.0173\n",
      "Epoch 5/10, Batch 1/494, Loss: 0.0110\n",
      "Epoch 5/10, Batch 11/494, Loss: 0.0163\n",
      "Epoch 5/10, Batch 21/494, Loss: 0.0110\n",
      "Epoch 5/10, Batch 31/494, Loss: 0.0206\n",
      "Epoch 5/10, Batch 41/494, Loss: 0.0157\n",
      "Epoch 5/10, Batch 51/494, Loss: 0.0216\n",
      "Epoch 5/10, Batch 61/494, Loss: 0.0172\n",
      "Epoch 5/10, Batch 71/494, Loss: 0.0164\n",
      "Epoch 5/10, Batch 81/494, Loss: 0.0159\n",
      "Epoch 5/10, Batch 91/494, Loss: 0.0146\n",
      "Epoch 5/10, Batch 101/494, Loss: 0.0189\n",
      "Epoch 5/10, Batch 111/494, Loss: 0.0206\n",
      "Epoch 5/10, Batch 121/494, Loss: 0.0072\n",
      "Epoch 5/10, Batch 131/494, Loss: 0.0080\n",
      "Epoch 5/10, Batch 141/494, Loss: 0.0108\n",
      "Epoch 5/10, Batch 151/494, Loss: 0.0196\n",
      "Epoch 5/10, Batch 161/494, Loss: 0.0132\n",
      "Epoch 5/10, Batch 171/494, Loss: 0.0125\n",
      "Epoch 5/10, Batch 181/494, Loss: 0.0116\n",
      "Epoch 5/10, Batch 191/494, Loss: 0.0099\n",
      "Epoch 5/10, Batch 201/494, Loss: 0.0116\n",
      "Epoch 5/10, Batch 211/494, Loss: 0.0168\n",
      "Epoch 5/10, Batch 221/494, Loss: 0.0151\n",
      "Epoch 5/10, Batch 231/494, Loss: 0.0128\n",
      "Epoch 5/10, Batch 241/494, Loss: 0.0109\n",
      "Epoch 5/10, Batch 251/494, Loss: 0.0195\n",
      "Epoch 5/10, Batch 261/494, Loss: 0.0183\n",
      "Epoch 5/10, Batch 271/494, Loss: 0.0038\n",
      "Epoch 5/10, Batch 281/494, Loss: 0.0141\n",
      "Epoch 5/10, Batch 291/494, Loss: 0.0112\n",
      "Epoch 5/10, Batch 301/494, Loss: 0.0239\n",
      "Epoch 5/10, Batch 311/494, Loss: 0.0113\n",
      "Epoch 5/10, Batch 321/494, Loss: 0.0148\n",
      "Epoch 5/10, Batch 331/494, Loss: 0.0178\n",
      "Epoch 5/10, Batch 341/494, Loss: 0.0181\n",
      "Epoch 5/10, Batch 351/494, Loss: 0.0157\n",
      "Epoch 5/10, Batch 361/494, Loss: 0.0184\n",
      "Epoch 5/10, Batch 371/494, Loss: 0.0162\n",
      "Epoch 5/10, Batch 381/494, Loss: 0.0161\n",
      "Epoch 5/10, Batch 391/494, Loss: 0.0124\n",
      "Epoch 5/10, Batch 401/494, Loss: 0.0162\n",
      "Epoch 5/10, Batch 411/494, Loss: 0.0124\n",
      "Epoch 5/10, Batch 421/494, Loss: 0.0156\n",
      "Epoch 5/10, Batch 431/494, Loss: 0.0168\n",
      "Epoch 5/10, Batch 441/494, Loss: 0.0116\n",
      "Epoch 5/10, Batch 451/494, Loss: 0.0152\n",
      "Epoch 5/10, Batch 461/494, Loss: 0.0107\n",
      "Epoch 5/10, Batch 471/494, Loss: 0.0159\n",
      "Epoch 5/10, Batch 481/494, Loss: 0.0130\n",
      "Epoch 5/10, Batch 491/494, Loss: 0.0102\n",
      "Epoch 5/10, Avg Training Loss: 0.0160\n",
      "Epoch 5/10, Validation Loss: 0.0185\n",
      "Epoch 6/10, Batch 1/494, Loss: 0.0153\n",
      "Epoch 6/10, Batch 11/494, Loss: 0.0170\n",
      "Epoch 6/10, Batch 21/494, Loss: 0.0169\n",
      "Epoch 6/10, Batch 31/494, Loss: 0.0146\n",
      "Epoch 6/10, Batch 41/494, Loss: 0.0114\n",
      "Epoch 6/10, Batch 51/494, Loss: 0.0149\n",
      "Epoch 6/10, Batch 61/494, Loss: 0.0106\n",
      "Epoch 6/10, Batch 71/494, Loss: 0.0117\n",
      "Epoch 6/10, Batch 81/494, Loss: 0.0110\n",
      "Epoch 6/10, Batch 91/494, Loss: 0.0157\n",
      "Epoch 6/10, Batch 101/494, Loss: 0.0165\n",
      "Epoch 6/10, Batch 111/494, Loss: 0.0165\n",
      "Epoch 6/10, Batch 121/494, Loss: 0.0123\n",
      "Epoch 6/10, Batch 131/494, Loss: 0.0160\n",
      "Epoch 6/10, Batch 141/494, Loss: 0.0229\n",
      "Epoch 6/10, Batch 151/494, Loss: 0.0153\n",
      "Epoch 6/10, Batch 161/494, Loss: 0.0146\n",
      "Epoch 6/10, Batch 171/494, Loss: 0.0081\n",
      "Epoch 6/10, Batch 181/494, Loss: 0.0077\n",
      "Epoch 6/10, Batch 191/494, Loss: 0.0182\n",
      "Epoch 6/10, Batch 201/494, Loss: 0.0198\n",
      "Epoch 6/10, Batch 211/494, Loss: 0.0212\n",
      "Epoch 6/10, Batch 221/494, Loss: 0.0116\n",
      "Epoch 6/10, Batch 231/494, Loss: 0.0103\n",
      "Epoch 6/10, Batch 241/494, Loss: 0.0205\n",
      "Epoch 6/10, Batch 251/494, Loss: 0.0151\n",
      "Epoch 6/10, Batch 261/494, Loss: 0.0195\n",
      "Epoch 6/10, Batch 271/494, Loss: 0.0152\n",
      "Epoch 6/10, Batch 281/494, Loss: 0.0128\n",
      "Epoch 6/10, Batch 291/494, Loss: 0.0107\n",
      "Epoch 6/10, Batch 301/494, Loss: 0.0102\n",
      "Epoch 6/10, Batch 311/494, Loss: 0.0119\n",
      "Epoch 6/10, Batch 321/494, Loss: 0.0080\n",
      "Epoch 6/10, Batch 331/494, Loss: 0.0138\n",
      "Epoch 6/10, Batch 341/494, Loss: 0.0142\n",
      "Epoch 6/10, Batch 351/494, Loss: 0.0140\n",
      "Epoch 6/10, Batch 361/494, Loss: 0.0131\n",
      "Epoch 6/10, Batch 371/494, Loss: 0.0183\n",
      "Epoch 6/10, Batch 381/494, Loss: 0.0130\n",
      "Epoch 6/10, Batch 391/494, Loss: 0.0205\n",
      "Epoch 6/10, Batch 401/494, Loss: 0.0153\n",
      "Epoch 6/10, Batch 411/494, Loss: 0.0164\n",
      "Epoch 6/10, Batch 421/494, Loss: 0.0142\n",
      "Epoch 6/10, Batch 431/494, Loss: 0.0028\n",
      "Epoch 6/10, Batch 441/494, Loss: 0.0150\n",
      "Epoch 6/10, Batch 451/494, Loss: 0.0161\n",
      "Epoch 6/10, Batch 461/494, Loss: 0.0153\n",
      "Epoch 6/10, Batch 471/494, Loss: 0.0169\n",
      "Epoch 6/10, Batch 481/494, Loss: 0.0169\n",
      "Epoch 6/10, Batch 491/494, Loss: 0.0151\n",
      "Epoch 6/10, Avg Training Loss: 0.0145\n",
      "Epoch 6/10, Validation Loss: 0.0144\n",
      "Epoch 7/10, Batch 1/494, Loss: 0.0187\n",
      "Epoch 7/10, Batch 11/494, Loss: 0.0158\n",
      "Epoch 7/10, Batch 21/494, Loss: 0.0153\n",
      "Epoch 7/10, Batch 31/494, Loss: 0.0064\n",
      "Epoch 7/10, Batch 41/494, Loss: 0.0149\n",
      "Epoch 7/10, Batch 51/494, Loss: 0.0122\n",
      "Epoch 7/10, Batch 61/494, Loss: 0.0183\n",
      "Epoch 7/10, Batch 71/494, Loss: 0.0181\n",
      "Epoch 7/10, Batch 81/494, Loss: 0.0089\n",
      "Epoch 7/10, Batch 91/494, Loss: 0.0118\n",
      "Epoch 7/10, Batch 101/494, Loss: 0.0106\n",
      "Epoch 7/10, Batch 111/494, Loss: 0.0162\n",
      "Epoch 7/10, Batch 121/494, Loss: 0.0117\n",
      "Epoch 7/10, Batch 131/494, Loss: 0.0068\n",
      "Epoch 7/10, Batch 141/494, Loss: 0.0115\n",
      "Epoch 7/10, Batch 151/494, Loss: 0.0108\n",
      "Epoch 7/10, Batch 161/494, Loss: 0.0188\n",
      "Epoch 7/10, Batch 171/494, Loss: 0.0066\n",
      "Epoch 7/10, Batch 181/494, Loss: 0.0123\n",
      "Epoch 7/10, Batch 191/494, Loss: 0.0143\n",
      "Epoch 7/10, Batch 201/494, Loss: 0.0140\n",
      "Epoch 7/10, Batch 211/494, Loss: 0.0118\n",
      "Epoch 7/10, Batch 221/494, Loss: 0.0108\n",
      "Epoch 7/10, Batch 231/494, Loss: 0.0182\n",
      "Epoch 7/10, Batch 241/494, Loss: 0.0156\n",
      "Epoch 7/10, Batch 251/494, Loss: 0.0146\n",
      "Epoch 7/10, Batch 261/494, Loss: 0.0140\n",
      "Epoch 7/10, Batch 271/494, Loss: 0.0183\n",
      "Epoch 7/10, Batch 281/494, Loss: 0.0182\n",
      "Epoch 7/10, Batch 291/494, Loss: 0.0121\n",
      "Epoch 7/10, Batch 301/494, Loss: 0.0151\n",
      "Epoch 7/10, Batch 311/494, Loss: 0.0176\n",
      "Epoch 7/10, Batch 321/494, Loss: 0.0157\n",
      "Epoch 7/10, Batch 331/494, Loss: 0.0160\n",
      "Epoch 7/10, Batch 341/494, Loss: 0.0138\n",
      "Epoch 7/10, Batch 351/494, Loss: 0.0113\n",
      "Epoch 7/10, Batch 361/494, Loss: 0.0073\n",
      "Epoch 7/10, Batch 371/494, Loss: 0.0163\n",
      "Epoch 7/10, Batch 381/494, Loss: 0.0164\n",
      "Epoch 7/10, Batch 391/494, Loss: 0.0175\n",
      "Epoch 7/10, Batch 401/494, Loss: 0.0102\n",
      "Epoch 7/10, Batch 411/494, Loss: 0.0166\n",
      "Epoch 7/10, Batch 421/494, Loss: 0.0169\n",
      "Epoch 7/10, Batch 431/494, Loss: 0.0150\n",
      "Epoch 7/10, Batch 441/494, Loss: 0.0143\n",
      "Epoch 7/10, Batch 451/494, Loss: 0.0114\n",
      "Epoch 7/10, Batch 461/494, Loss: 0.0130\n",
      "Epoch 7/10, Batch 471/494, Loss: 0.0109\n",
      "Epoch 7/10, Batch 481/494, Loss: 0.0121\n",
      "Epoch 7/10, Batch 491/494, Loss: 0.0074\n",
      "Epoch 7/10, Avg Training Loss: 0.0137\n",
      "Epoch 7/10, Validation Loss: 0.0138\n",
      "Epoch 8/10, Batch 1/494, Loss: 0.0135\n",
      "Epoch 8/10, Batch 11/494, Loss: 0.0128\n",
      "Epoch 8/10, Batch 21/494, Loss: 0.0145\n",
      "Epoch 8/10, Batch 31/494, Loss: 0.0154\n",
      "Epoch 8/10, Batch 41/494, Loss: 0.0114\n",
      "Epoch 8/10, Batch 51/494, Loss: 0.0072\n",
      "Epoch 8/10, Batch 61/494, Loss: 0.0123\n",
      "Epoch 8/10, Batch 71/494, Loss: 0.0117\n",
      "Epoch 8/10, Batch 81/494, Loss: 0.0083\n",
      "Epoch 8/10, Batch 91/494, Loss: 0.0112\n",
      "Epoch 8/10, Batch 101/494, Loss: 0.0144\n",
      "Epoch 8/10, Batch 111/494, Loss: 0.0084\n",
      "Epoch 8/10, Batch 121/494, Loss: 0.0185\n",
      "Epoch 8/10, Batch 131/494, Loss: 0.0037\n",
      "Epoch 8/10, Batch 141/494, Loss: 0.0129\n",
      "Epoch 8/10, Batch 151/494, Loss: 0.0184\n",
      "Epoch 8/10, Batch 161/494, Loss: 0.0142\n",
      "Epoch 8/10, Batch 171/494, Loss: 0.0143\n",
      "Epoch 8/10, Batch 181/494, Loss: 0.0094\n",
      "Epoch 8/10, Batch 191/494, Loss: 0.0135\n",
      "Epoch 8/10, Batch 201/494, Loss: 0.0151\n",
      "Epoch 8/10, Batch 211/494, Loss: 0.0103\n",
      "Epoch 8/10, Batch 221/494, Loss: 0.0127\n",
      "Epoch 8/10, Batch 231/494, Loss: 0.0100\n",
      "Epoch 8/10, Batch 241/494, Loss: 0.0155\n",
      "Epoch 8/10, Batch 251/494, Loss: 0.0191\n",
      "Epoch 8/10, Batch 261/494, Loss: 0.0130\n",
      "Epoch 8/10, Batch 271/494, Loss: 0.0117\n",
      "Epoch 8/10, Batch 281/494, Loss: 0.0069\n",
      "Epoch 8/10, Batch 291/494, Loss: 0.0155\n",
      "Epoch 8/10, Batch 301/494, Loss: 0.0161\n",
      "Epoch 8/10, Batch 311/494, Loss: 0.0119\n",
      "Epoch 8/10, Batch 321/494, Loss: 0.0119\n",
      "Epoch 8/10, Batch 331/494, Loss: 0.0098\n",
      "Epoch 8/10, Batch 341/494, Loss: 0.0128\n",
      "Epoch 8/10, Batch 351/494, Loss: 0.0124\n",
      "Epoch 8/10, Batch 361/494, Loss: 0.0172\n",
      "Epoch 8/10, Batch 371/494, Loss: 0.0116\n",
      "Epoch 8/10, Batch 381/494, Loss: 0.0138\n",
      "Epoch 8/10, Batch 391/494, Loss: 0.0153\n",
      "Epoch 8/10, Batch 401/494, Loss: 0.0122\n",
      "Epoch 8/10, Batch 411/494, Loss: 0.0138\n",
      "Epoch 8/10, Batch 421/494, Loss: 0.0118\n",
      "Epoch 8/10, Batch 431/494, Loss: 0.0066\n",
      "Epoch 8/10, Batch 441/494, Loss: 0.0067\n",
      "Epoch 8/10, Batch 451/494, Loss: 0.0137\n",
      "Epoch 8/10, Batch 461/494, Loss: 0.0127\n",
      "Epoch 8/10, Batch 471/494, Loss: 0.0125\n",
      "Epoch 8/10, Batch 481/494, Loss: 0.0087\n",
      "Epoch 8/10, Batch 491/494, Loss: 0.0098\n",
      "Epoch 8/10, Avg Training Loss: 0.0129\n",
      "Epoch 8/10, Validation Loss: 0.0134\n",
      "Epoch 9/10, Batch 1/494, Loss: 0.0095\n",
      "Epoch 9/10, Batch 11/494, Loss: 0.0159\n",
      "Epoch 9/10, Batch 21/494, Loss: 0.0125\n",
      "Epoch 9/10, Batch 31/494, Loss: 0.0124\n",
      "Epoch 9/10, Batch 41/494, Loss: 0.0087\n",
      "Epoch 9/10, Batch 51/494, Loss: 0.0114\n",
      "Epoch 9/10, Batch 61/494, Loss: 0.0158\n",
      "Epoch 9/10, Batch 71/494, Loss: 0.0133\n",
      "Epoch 9/10, Batch 81/494, Loss: 0.0105\n",
      "Epoch 9/10, Batch 91/494, Loss: 0.0161\n",
      "Epoch 9/10, Batch 101/494, Loss: 0.0109\n",
      "Epoch 9/10, Batch 111/494, Loss: 0.0131\n",
      "Epoch 9/10, Batch 121/494, Loss: 0.0161\n",
      "Epoch 9/10, Batch 131/494, Loss: 0.0155\n",
      "Epoch 9/10, Batch 141/494, Loss: 0.0126\n",
      "Epoch 9/10, Batch 151/494, Loss: 0.0055\n",
      "Epoch 9/10, Batch 161/494, Loss: 0.0121\n",
      "Epoch 9/10, Batch 171/494, Loss: 0.0168\n",
      "Epoch 9/10, Batch 181/494, Loss: 0.0146\n",
      "Epoch 9/10, Batch 191/494, Loss: 0.0169\n",
      "Epoch 9/10, Batch 201/494, Loss: 0.0172\n",
      "Epoch 9/10, Batch 211/494, Loss: 0.0153\n",
      "Epoch 9/10, Batch 221/494, Loss: 0.0109\n",
      "Epoch 9/10, Batch 231/494, Loss: 0.0171\n",
      "Epoch 9/10, Batch 241/494, Loss: 0.0121\n",
      "Epoch 9/10, Batch 251/494, Loss: 0.0139\n",
      "Epoch 9/10, Batch 261/494, Loss: 0.0076\n",
      "Epoch 9/10, Batch 271/494, Loss: 0.0164\n",
      "Epoch 9/10, Batch 281/494, Loss: 0.0154\n",
      "Epoch 9/10, Batch 291/494, Loss: 0.0170\n",
      "Epoch 9/10, Batch 301/494, Loss: 0.0100\n",
      "Epoch 9/10, Batch 311/494, Loss: 0.0126\n",
      "Epoch 9/10, Batch 321/494, Loss: 0.0131\n",
      "Epoch 9/10, Batch 331/494, Loss: 0.0160\n",
      "Epoch 9/10, Batch 341/494, Loss: 0.0160\n",
      "Epoch 9/10, Batch 351/494, Loss: 0.0155\n",
      "Epoch 9/10, Batch 361/494, Loss: 0.0140\n",
      "Epoch 9/10, Batch 371/494, Loss: 0.0128\n",
      "Epoch 9/10, Batch 381/494, Loss: 0.0137\n",
      "Epoch 9/10, Batch 391/494, Loss: 0.0168\n",
      "Epoch 9/10, Batch 401/494, Loss: 0.0133\n",
      "Epoch 9/10, Batch 411/494, Loss: 0.0164\n",
      "Epoch 9/10, Batch 421/494, Loss: 0.0062\n",
      "Epoch 9/10, Batch 431/494, Loss: 0.0158\n",
      "Epoch 9/10, Batch 441/494, Loss: 0.0126\n",
      "Epoch 9/10, Batch 451/494, Loss: 0.0131\n",
      "Epoch 9/10, Batch 461/494, Loss: 0.0095\n",
      "Epoch 9/10, Batch 471/494, Loss: 0.0138\n",
      "Epoch 9/10, Batch 481/494, Loss: 0.0119\n",
      "Epoch 9/10, Batch 491/494, Loss: 0.0118\n",
      "Epoch 9/10, Avg Training Loss: 0.0126\n",
      "Epoch 9/10, Validation Loss: 0.0128\n",
      "Epoch 10/10, Batch 1/494, Loss: 0.0100\n",
      "Epoch 10/10, Batch 11/494, Loss: 0.0120\n",
      "Epoch 10/10, Batch 21/494, Loss: 0.0147\n",
      "Epoch 10/10, Batch 31/494, Loss: 0.0135\n",
      "Epoch 10/10, Batch 41/494, Loss: 0.0142\n",
      "Epoch 10/10, Batch 51/494, Loss: 0.0140\n",
      "Epoch 10/10, Batch 61/494, Loss: 0.0084\n",
      "Epoch 10/10, Batch 71/494, Loss: 0.0132\n",
      "Epoch 10/10, Batch 81/494, Loss: 0.0121\n",
      "Epoch 10/10, Batch 91/494, Loss: 0.0154\n",
      "Epoch 10/10, Batch 101/494, Loss: 0.0109\n",
      "Epoch 10/10, Batch 111/494, Loss: 0.0144\n",
      "Epoch 10/10, Batch 121/494, Loss: 0.0126\n",
      "Epoch 10/10, Batch 131/494, Loss: 0.0151\n",
      "Epoch 10/10, Batch 141/494, Loss: 0.0116\n",
      "Epoch 10/10, Batch 151/494, Loss: 0.0136\n",
      "Epoch 10/10, Batch 161/494, Loss: 0.0100\n",
      "Epoch 10/10, Batch 171/494, Loss: 0.0158\n",
      "Epoch 10/10, Batch 181/494, Loss: 0.0133\n",
      "Epoch 10/10, Batch 191/494, Loss: 0.0128\n",
      "Epoch 10/10, Batch 201/494, Loss: 0.0162\n",
      "Epoch 10/10, Batch 211/494, Loss: 0.0135\n",
      "Epoch 10/10, Batch 221/494, Loss: 0.0103\n",
      "Epoch 10/10, Batch 231/494, Loss: 0.0097\n",
      "Epoch 10/10, Batch 241/494, Loss: 0.0134\n",
      "Epoch 10/10, Batch 251/494, Loss: 0.0166\n",
      "Epoch 10/10, Batch 261/494, Loss: 0.0136\n",
      "Epoch 10/10, Batch 271/494, Loss: 0.0118\n",
      "Epoch 10/10, Batch 281/494, Loss: 0.0098\n",
      "Epoch 10/10, Batch 291/494, Loss: 0.0146\n",
      "Epoch 10/10, Batch 301/494, Loss: 0.0075\n",
      "Epoch 10/10, Batch 311/494, Loss: 0.0103\n",
      "Epoch 10/10, Batch 321/494, Loss: 0.0103\n",
      "Epoch 10/10, Batch 331/494, Loss: 0.0118\n",
      "Epoch 10/10, Batch 341/494, Loss: 0.0099\n",
      "Epoch 10/10, Batch 351/494, Loss: 0.0058\n",
      "Epoch 10/10, Batch 361/494, Loss: 0.0116\n",
      "Epoch 10/10, Batch 371/494, Loss: 0.0088\n",
      "Epoch 10/10, Batch 381/494, Loss: 0.0159\n",
      "Epoch 10/10, Batch 391/494, Loss: 0.0112\n",
      "Epoch 10/10, Batch 401/494, Loss: 0.0125\n",
      "Epoch 10/10, Batch 411/494, Loss: 0.0142\n",
      "Epoch 10/10, Batch 421/494, Loss: 0.0128\n",
      "Epoch 10/10, Batch 431/494, Loss: 0.0138\n",
      "Epoch 10/10, Batch 441/494, Loss: 0.0127\n",
      "Epoch 10/10, Batch 451/494, Loss: 0.0129\n",
      "Epoch 10/10, Batch 461/494, Loss: 0.0042\n",
      "Epoch 10/10, Batch 471/494, Loss: 0.0137\n",
      "Epoch 10/10, Batch 481/494, Loss: 0.0116\n",
      "Epoch 10/10, Batch 491/494, Loss: 0.0163\n",
      "Epoch 10/10, Avg Training Loss: 0.0123\n",
      "Epoch 10/10, Validation Loss: 0.0140\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "train(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5c044a2-11f5-4634-83b6-b90cf53858eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "torch.save(model.state_dict(), \"deeplabv3_binary_segmentation_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e1030a-a6cf-4711-bb8a-077c5f63f7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calviet/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/calviet/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_3811188/909945982.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('deeplabv3_binary_segmentation_final.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepLabV3(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): DeepLabHead(\n",
       "    (0): ASPP(\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (3): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (4): ASPPPooling(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (project): Sequential(\n",
       "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "\n",
    "# Load the model architecture\n",
    "def get_model(num_classes=1):\n",
    "    model = deeplabv3_resnet50(pretrained=False)\n",
    "    model.classifier[4] = torch.nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "    return model\n",
    "\n",
    "# Instantiate and load weights\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = get_model()\n",
    "model.load_state_dict(torch.load('deeplabv3_binary_segmentation_final.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ee08ef9-3f5f-4c14-b377-b177811b8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def test_model(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    total_pixels = 0\n",
    "    correct_pixels = 0\n",
    "    intersection = 0\n",
    "    union = 0\n",
    "    dice_total = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            outputs = model(images)['out']\n",
    "            preds = torch.sigmoid(outputs) > 0.5  # Boolean prediction\n",
    "\n",
    "            preds = preds.squeeze(1).bool()\n",
    "            masks = masks.squeeze(1).bool()\n",
    "\n",
    "            correct_pixels += (preds == masks).sum().item()\n",
    "            total_pixels += torch.numel(preds)\n",
    "\n",
    "            inter = (preds & masks).sum().item()\n",
    "            union_area = (preds | masks).sum().item()\n",
    "            intersection += inter\n",
    "            union += union_area\n",
    "\n",
    "            # Dice score for this batch\n",
    "            dice = (2 * inter) / (preds.sum().item() + masks.sum().item() + 1e-8)\n",
    "            dice_total += dice\n",
    "            num_batches += 1\n",
    "\n",
    "    accuracy = correct_pixels / total_pixels\n",
    "    iou = intersection / (union + 1e-8)\n",
    "    dice_score = dice_total / num_batches\n",
    "\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"IoU: {iou:.4f}\")\n",
    "    print(f\"Dice Score: {dice_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "282988ab-b560-4555-8c13-b6bdf1fd76df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 247/247 [02:35<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9935\n",
      "IoU: 0.9286\n",
      "Dice Score: 0.9463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now call the test function\n",
    "test_model(model, val_loader, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
